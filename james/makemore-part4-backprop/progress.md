# Makemore Part 4: Becoming a Backprop Ninja Progress Tracker

## Video: Building makemore Part 4: Becoming a Backprop Ninja
**Duration:** 1h55m

**Description:** We take the 2-layer MLP (with BatchNorm) from the previous video and backpropagate through it manually without using PyTorch autograd's loss.backward(): through the cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table. Along the way, we get a strong intuitive understanding about how gradients flow backwards through the compute graph and on the level of efficient Tensors, not just individual scalars like in micrograd.

## Progress Score: 0/10

### Learning Objectives:
- [ ] Manually implement backpropagation without PyTorch autograd
- [ ] Understand gradient flow through cross entropy loss
- [ ] Backpropagate through linear layers, tanh, and batchnorm
- [ ] Work with efficient Tensor-level gradients
- [ ] Build intuition for neural network optimization
- [ ] Debug modern neural networks effectively

### Notes:
*Add your notes and insights here as you progress through the video*

### Completed Sections:
*Track which parts of the video you've completed*

### Questions/Challenges:
*Note any questions or challenging concepts* 