# Makemore Part 3: Activations & Gradients, BatchNorm Progress Tracker

## Video: Building makemore Part 3: Activations & Gradients, BatchNorm
**Duration:** 1h55m

**Description:** We dive into some of the internals of MLPs with multiple layers and scrutinize the statistics of the forward pass activations, backward pass gradients, and some of the pitfalls when they are improperly scaled. We also look at the typical diagnostic tools and visualizations you'd want to use to understand the health of your deep network. We learn why training deep neural nets can be fragile and introduce the first modern innovation that made doing so much easier: Batch Normalization.

## Progress Score: 0/10

### Learning Objectives:
- [ ] Analyze forward pass activations statistics
- [ ] Understand backward pass gradients
- [ ] Identify scaling pitfalls in deep networks
- [ ] Use diagnostic tools and visualizations
- [ ] Understand why deep network training is fragile
- [ ] Implement Batch Normalization

### Notes:
*Add your notes and insights here as you progress through the video*

### Completed Sections:
*Track which parts of the video you've completed*

### Questions/Challenges:
*Note any questions or challenging concepts* 