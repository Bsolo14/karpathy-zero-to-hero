# GPT Tokenizer Progress Tracker

## Video: Let's build the GPT Tokenizer
**Duration:** 2h13m

**Description:** The Tokenizer is a necessary and pervasive component of Large Language Models (LLMs), where it translates between strings and tokens (text chunks). Tokenizers are a completely separate stage of the LLM pipeline: they have their own training sets, training algorithms (Byte Pair Encoding), and after training implement two fundamental functions: encode() from strings to tokens, and decode() back from tokens to strings. In this lecture we build from scratch the Tokenizer used in the GPT series from OpenAI.

## Progress Score: 0/10

### Learning Objectives:
- [ ] Understand tokenization in Large Language Models
- [ ] Learn Byte Pair Encoding (BPE) algorithm
- [ ] Implement encode() function (strings to tokens)
- [ ] Implement decode() function (tokens to strings)
- [ ] Build GPT tokenizer from scratch
- [ ] Understand tokenization issues and problems in LLMs

### Notes:
*Add your notes and insights here as you progress through the video*

### Completed Sections:
*Track which parts of the video you've completed*

### Questions/Challenges:
*Note any questions or challenging concepts* 